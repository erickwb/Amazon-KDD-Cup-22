{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "-> Objetivo: Dada uma consulta especificada pelo usuário e uma lista de produtos correspondentes, o objetivo desta tarefa é classificar os produtos para que os produtos relevantes sejam classificados acima dos não relevantes.\n",
    "\n",
    "A entrada para esta tarefa será uma lista de consultas com seus identificadores. O sistema terá que gerar um arquivo CSV onde o **query_id** estará na primeira coluna e o **product_id** na segunda coluna, onde para cada **query_id** primeira linha será o produto mais relevante e a última linha o produto menos relevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximos Passos \n",
    "\n",
    "1 Criar seçao de metricas\n",
    "\n",
    "2 criar modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/erick.correia/.local/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: numpy in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.22.3)\n",
      "Requirement already satisfied: scipy in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: annoy in /home/erick.correia/.local/lib/python3.8/site-packages (1.17.1)\n",
      "Requirement already satisfied: faiss-cpu in /home/erick.correia/.local/lib/python3.8/site-packages (1.7.2)\n",
      "Requirement already up-to-date: sentence-transformers in /home/erick.correia/.local/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=4.6.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (4.21.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.22.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub>=0.4.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.8.17)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/erick.correia/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision->sentence-transformers) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /home/erick.correia/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
      "Requirement already satisfied: nltk in /home/erick.correia/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (2022.8.17)\n",
      "Requirement already satisfied: tqdm in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install annoy\n",
    "!pip install faiss-cpu\n",
    "!pip install -U sentence-transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "#metricas\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "#modelos\n",
    "import lightgbm as lgb\n",
    "import faiss\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train-v0.3.csv')\n",
    "product = pd.read_csv('dataset/product_catalogue-v0.3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train data =  (781744, 5)\n",
      "shape train product =  (883868, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape train data = \" , train_df.shape)\n",
    "print(\"shape train product = \" , product.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 883868 entries, 0 to 883867\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   product_id            883868 non-null  object\n",
      " 1   product_title         883719 non-null  object\n",
      " 2   product_description   446710 non-null  object\n",
      " 3   product_bullet_point  739196 non-null  object\n",
      " 4   product_brand         807927 non-null  object\n",
      " 5   product_color_name    533660 non-null  object\n",
      " 6   product_locale        883868 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 47.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color_name</th>\n",
       "      <th>product_locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0188A3QRM</td>\n",
       "      <td>Amazon Basics Woodcased #2 Pencils, Unsharpene...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144 woodcase #2 HB pencils made from high-qual...</td>\n",
       "      <td>Amazon Basics</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B075VXJ9VG</td>\n",
       "      <td>BAZIC Pencil #2 HB Pencils, Latex Free Eraser,...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;BACK TO BAZIC&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Our go...</td>\n",
       "      <td>&amp;#11088; UN-SHARPENED #2 PREMIUM PENCILS. Each...</td>\n",
       "      <td>BAZIC Products</td>\n",
       "      <td>12-count</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07G7F6JZ6</td>\n",
       "      <td>Emraw Pre Sharpened Round Primary Size No 2 Ju...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;Emraw Pre-Sharpened #2 HB Wood Pencils -...</td>\n",
       "      <td>✓ PACK OF 8 NUMBER 2 PRESHARPENED BEGINNERS PE...</td>\n",
       "      <td>Emraw</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07JZJLHCF</td>\n",
       "      <td>Emraw Pre Sharpened Triangular Primary Size No...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;Emraw Pre-Sharpened #2 HB Wood Pencils -...</td>\n",
       "      <td>✓ PACK OF 6 NUMBER 2 PRESHARPENED BEGINNERS PE...</td>\n",
       "      <td>Emraw</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07MGKC3DD</td>\n",
       "      <td>BIC Evolution Cased Pencil, #2 Lead, Gray Barr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Premium #2 HB lead pencils with break-resistan...</td>\n",
       "      <td>Design House</td>\n",
       "      <td>Gray</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id                                      product_title  \\\n",
       "0  B0188A3QRM  Amazon Basics Woodcased #2 Pencils, Unsharpene...   \n",
       "1  B075VXJ9VG  BAZIC Pencil #2 HB Pencils, Latex Free Eraser,...   \n",
       "2  B07G7F6JZ6  Emraw Pre Sharpened Round Primary Size No 2 Ju...   \n",
       "3  B07JZJLHCF  Emraw Pre Sharpened Triangular Primary Size No...   \n",
       "4  B07MGKC3DD  BIC Evolution Cased Pencil, #2 Lead, Gray Barr...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0                                                NaN   \n",
       "1  <p><strong>BACK TO BAZIC</strong></p><p>Our go...   \n",
       "2  <p><b>Emraw Pre-Sharpened #2 HB Wood Pencils -...   \n",
       "3  <p><b>Emraw Pre-Sharpened #2 HB Wood Pencils -...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                product_bullet_point   product_brand  \\\n",
       "0  144 woodcase #2 HB pencils made from high-qual...   Amazon Basics   \n",
       "1  &#11088; UN-SHARPENED #2 PREMIUM PENCILS. Each...  BAZIC Products   \n",
       "2  ✓ PACK OF 8 NUMBER 2 PRESHARPENED BEGINNERS PE...           Emraw   \n",
       "3  ✓ PACK OF 6 NUMBER 2 PRESHARPENED BEGINNERS PE...           Emraw   \n",
       "4  Premium #2 HB lead pencils with break-resistan...    Design House   \n",
       "\n",
       "  product_color_name product_locale  \n",
       "0             Yellow             us  \n",
       "1           12-count             us  \n",
       "2             Yellow             us  \n",
       "3             Yellow             us  \n",
       "4               Gray             us  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.info()\n",
    "product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 781744 entries, 0 to 781743\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   query_id      781744 non-null  int64 \n",
      " 1   query         781744 non-null  object\n",
      " 2   query_locale  781744 non-null  object\n",
      " 3   product_id    781744 non-null  object\n",
      " 4   esci_label    781744 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 29.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_locale</th>\n",
       "      <th>product_id</th>\n",
       "      <th>esci_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0002LCZV4</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B00125Q75Y</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B001AZ1D3C</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B001B097KC</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                      query query_locale  product_id esci_label\n",
       "0         0  # 2 pencils not sharpened           us  B0000AQO0O      exact\n",
       "1         0  # 2 pencils not sharpened           us  B0002LCZV4      exact\n",
       "2         0  # 2 pencils not sharpened           us  B00125Q75Y      exact\n",
       "3         0  # 2 pencils not sharpened           us  B001AZ1D3C      exact\n",
       "4         0  # 2 pencils not sharpened           us  B001B097KC      exact"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_id        0\n",
       "query           0\n",
       "query_locale    0\n",
       "product_id      0\n",
       "esci_label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id                   0\n",
       "product_title              149\n",
       "product_description     437158\n",
       "product_bullet_point    144672\n",
       "product_brand            75941\n",
       "product_color_name      350208\n",
       "product_locale               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> separar o produtos com o locale = us\n",
    "\n",
    "-> colocar o product_title no train dataset de treino\n",
    "\n",
    "-> fazer o tratamendo na coluna de title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pd.merge(train_df, \n",
    "                          product[[\"product_id\", \"product_locale\", \"product_title\"]],\n",
    "                          left_on=[\"product_id\", \"query_locale\"], \n",
    "                          right_on=[\"product_id\", \"product_locale\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(781744, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_locale</th>\n",
       "      <th>product_id</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>product_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>exact</td>\n",
       "      <td>us</td>\n",
       "      <td>Ticonderoga Beginner Pencils, Wood-Cased #2 HB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8799</td>\n",
       "      <td>pencils for kindergarteners</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>exact</td>\n",
       "      <td>us</td>\n",
       "      <td>Ticonderoga Beginner Pencils, Wood-Cased #2 HB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14844</td>\n",
       "      <td>#2 dixon oriole pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>substitute</td>\n",
       "      <td>us</td>\n",
       "      <td>Ticonderoga Beginner Pencils, Wood-Cased #2 HB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15768</td>\n",
       "      <td>#2 pencils with erasers sharpened not soft</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>substitute</td>\n",
       "      <td>us</td>\n",
       "      <td>Ticonderoga Beginner Pencils, Wood-Cased #2 HB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16972</td>\n",
       "      <td>classroom friendly supplies pencil sharpener</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>us</td>\n",
       "      <td>Ticonderoga Beginner Pencils, Wood-Cased #2 HB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                         query query_locale  \\\n",
       "0         0                     # 2 pencils not sharpened           us   \n",
       "1      8799                   pencils for kindergarteners           us   \n",
       "2     14844         #2 dixon oriole pencils not sharpened           us   \n",
       "3     15768    #2 pencils with erasers sharpened not soft           us   \n",
       "4     16972  classroom friendly supplies pencil sharpener           us   \n",
       "\n",
       "   product_id  esci_label product_locale  \\\n",
       "0  B0000AQO0O       exact             us   \n",
       "1  B0000AQO0O       exact             us   \n",
       "2  B0000AQO0O  substitute             us   \n",
       "3  B0000AQO0O  substitute             us   \n",
       "4  B0000AQO0O  irrelevant             us   \n",
       "\n",
       "                                       product_title  \n",
       "0  Ticonderoga Beginner Pencils, Wood-Cased #2 HB...  \n",
       "1  Ticonderoga Beginner Pencils, Wood-Cased #2 HB...  \n",
       "2  Ticonderoga Beginner Pencils, Wood-Cased #2 HB...  \n",
       "3  Ticonderoga Beginner Pencils, Wood-Cased #2 HB...  \n",
       "4  Ticonderoga Beginner Pencils, Wood-Cased #2 HB...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(full_train.shape)\n",
    "full_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removendo StopWords / lowcase / caracteres especiais** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_df['query']\n",
    "text = text[:419730]\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            pencils sharpened\n",
      "1            pencils sharpened\n",
      "2            pencils sharpened\n",
      "3            pencils sharpened\n",
      "4            pencils sharpened\n",
      "                  ...         \n",
      "419725    zephyr polishing kit\n",
      "419726    zephyr polishing kit\n",
      "419727    zephyr polishing kit\n",
      "419728    zephyr polishing kit\n",
      "419729    zephyr polishing kit\n",
      "Name: query, Length: 419730, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10204/165082349.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace('[#,@,&,1-9]','')\n"
     ]
    }
   ],
   "source": [
    "#print(text)\n",
    "#text_tokens = word_tokenize(text)\n",
    "text= text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "#remove lowcase\n",
    "text = text.str.lower()\n",
    "#revome caracteres especiais \n",
    "text = text.str.replace('[#,@,&,1-9]','')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando vetores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = train_df['query'].tolist()\n",
    "sentences = text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and NaN\n",
    "#sentences = [word for word in list(set(sentences)) if type(word) is str]\n",
    "sentences = [word for word in list(set(sentences)) if type(word) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick.correia/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# initialize sentence transformer model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens') \n",
    "#paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20815, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sentence embeddings\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamento de dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndexFlatL2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = sentence_embeddings.shape[1]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20815"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste\n",
    "k = 4\n",
    "xq = model.encode([\"envelopes without security tint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3470 16750 17587 20170]]\n",
      "CPU times: user 27.4 ms, sys: 0 ns, total: 27.4 ms\n",
      "Wall time: 24.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'note cards without envelopes'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df['query'].iloc[[19805, 4829, 978, 14630]]\n",
    "#10620 14711  1269 15458\n",
    "sentences[3470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 4 vectors to return (k) - so we initialize a zero array to hold them\n",
    "vecs = np.zeros((k, size))\n",
    "# then iterate through each ID from I and add the reconstructed vector to our zero-array\n",
    "for i, val in enumerate(I[0].tolist()):\n",
    "    vecs[i, :] = index.reconstruct(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43119857,  0.91011864,  0.4051747 , -0.06226606,  0.09338088,\n",
       "       -0.20125297,  1.1626184 , -0.08322101, -0.36759871,  0.20486532,\n",
       "       -0.93038219, -0.61459619,  0.4171924 ,  0.51536101,  0.88794357,\n",
       "        0.45378926, -0.57322139,  0.27390629,  0.29525098, -0.19188608,\n",
       "       -0.24476814,  0.57090127,  0.04908442, -1.19785392, -0.54007924,\n",
       "       -0.35953718,  0.39977959, -0.30137578, -1.15571201,  0.45229897,\n",
       "        0.232692  ,  0.55188435,  1.23124671,  0.08548499,  1.41238093,\n",
       "       -0.40255302, -0.1310015 , -1.07996678, -0.15451427, -0.60550433,\n",
       "       -0.06073511,  0.32497635,  0.32338566,  0.35931736,  0.13488162,\n",
       "       -0.05715447,  1.5942353 ,  0.02342471, -0.78752214,  0.07201277,\n",
       "        0.56621379, -0.50664479,  1.9796375 ,  1.1744287 , -0.15058279,\n",
       "       -0.67751205,  0.21461996, -0.77990431,  0.93161666, -0.23863807,\n",
       "        0.13793547,  0.04795064, -0.26176697,  0.12465028, -0.2205309 ,\n",
       "       -0.32104367,  0.07732798,  0.5417518 , -0.53027701, -1.18944609,\n",
       "        0.67966878, -0.5132727 , -0.14703546, -0.69612658, -0.98438799,\n",
       "        0.2500239 ,  0.58343023,  0.5823316 , -0.31648138,  0.29999939,\n",
       "        0.20667967,  0.54710138,  0.16938254, -0.61964518, -0.57100695,\n",
       "        0.13071777,  0.50421041, -0.23905215, -0.47290611, -0.06526265,\n",
       "        0.51305205,  0.67895544, -0.10349973, -0.13229042, -1.06356549,\n",
       "        0.15988703, -0.43887404,  1.17856085,  0.13287328,  0.24475218])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementando um indice plano faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x = faiss.IndexFlatIP(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = model.encode([\"car paint\", \"arsoft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12983 13585 18938  3294  9513 16141  1653 11524 15252 13054]\n",
      " [ 9036  9111  9221 11956  6503 19832  1868   175 15698 17643]]\n"
     ]
    }
   ],
   "source": [
    "#Teste\n",
    "k = 10\n",
    "D, I = index_x.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es shoes skateboard'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df['query'].iloc[[10365, 12349, 7469, 7340]]\n",
    "#12516 15213  7553 12372 18626 15555 11820 20000  5935  4487\n",
    "sentences[7553]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13585 18938 11584 12983 14988  1653  3294 16141  9513 15252]\n",
      " [16613 15698  9111  6503 11043  9036 10212 19436 19188 19832]]\n"
     ]
    }
   ],
   "source": [
    "nbits = size*4  # resolution of bucketed vectors\n",
    "# initialize index and add vectors\n",
    "index_e = faiss.IndexLSH(size, nbits)\n",
    "index_e.add(sentence_embeddings)\n",
    "# and search\n",
    "D, I = index_e.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baby boy easter outfit'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df['query'].iloc[[12349, 7469, 9072, 10365, 4282, 2151, 7340, 50, 11175, 2665 ]]\n",
    "#[15213  7553  2488 12516  4900 11820 12372 15555 18626  5935\n",
    "sentences[4900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13585 18938 12983 11584 15252  3294  9513 11524  1653 17401]\n",
      " [  175  9111 16613 15698 11043  6503 19188 19436 10212 20444]]\n"
     ]
    }
   ],
   "source": [
    "# set HNSW index parameters\n",
    "M = 64  # number of connections each vertex will have\n",
    "ef_search = 32  # depth of layers explored during search\n",
    "ef_construction = 64  # depth of layers explored during index construction\n",
    "\n",
    "# initialize index (d == 128)\n",
    "index_h = faiss.IndexHNSWFlat(size, M)\n",
    "# set efConstruction and efSearch parameters\n",
    "index_h.hnsw.efConstruction = ef_construction\n",
    "index_h.hnsw.efSearch = ef_search\n",
    "# add data to index\n",
    "index_h.add(xq)\n",
    "\n",
    "# search as usual\n",
    "D, I = index.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bugsy blu ray'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
