{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "-> Objetivo: Dada uma consulta especificada pelo usuário e uma lista de produtos correspondentes, o objetivo desta tarefa é classificar os produtos para que os produtos relevantes sejam classificados acima dos não relevantes.\n",
    "\n",
    "A entrada para esta tarefa será uma lista de consultas com seus identificadores. O sistema terá que gerar um arquivo CSV onde o **query_id** estará na primeira coluna e o **product_id** na segunda coluna, onde para cada **query_id** primeira linha será o produto mais relevante e a última linha o produto menos relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/erick.correia/.local/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: numpy in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.22.3)\n",
      "Requirement already satisfied: scipy in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: annoy in /home/erick.correia/.local/lib/python3.8/site-packages (1.17.1)\n",
      "Requirement already satisfied: faiss-cpu in /home/erick.correia/.local/lib/python3.8/site-packages (1.7.2)\n",
      "Requirement already up-to-date: sentence-transformers in /home/erick.correia/.local/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=4.6.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (4.21.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.22.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub>=0.4.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.8.17)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/erick.correia/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision->sentence-transformers) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /home/erick.correia/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
      "Requirement already satisfied: nltk in /home/erick.correia/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (2022.8.17)\n",
      "Requirement already satisfied: tqdm in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install annoy\n",
    "!pip install faiss-cpu\n",
    "!pip install -U sentence-transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "import lightgbm as lgb\n",
    "import faiss\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train-v0.3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train data =  (781744, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape train data = \" , train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 781744 entries, 0 to 781743\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   query_id      781744 non-null  int64 \n",
      " 1   query         781744 non-null  object\n",
      " 2   query_locale  781744 non-null  object\n",
      " 3   product_id    781744 non-null  object\n",
      " 4   esci_label    781744 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 29.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_locale</th>\n",
       "      <th>product_id</th>\n",
       "      <th>esci_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0002LCZV4</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B00125Q75Y</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B001AZ1D3C</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B001B097KC</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                      query query_locale  product_id esci_label\n",
       "0         0  # 2 pencils not sharpened           us  B0000AQO0O      exact\n",
       "1         0  # 2 pencils not sharpened           us  B0002LCZV4      exact\n",
       "2         0  # 2 pencils not sharpened           us  B00125Q75Y      exact\n",
       "3         0  # 2 pencils not sharpened           us  B001AZ1D3C      exact\n",
       "4         0  # 2 pencils not sharpened           us  B001B097KC      exact"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_id        0\n",
       "query           0\n",
       "query_locale    0\n",
       "product_id      0\n",
       "esci_label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removendo StopWords / lowcase / caracteres especiais** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_df['query']\n",
    "text = text[:419730]\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            pencils sharpened\n",
      "1            pencils sharpened\n",
      "2            pencils sharpened\n",
      "3            pencils sharpened\n",
      "4            pencils sharpened\n",
      "                  ...         \n",
      "419725    zephyr polishing kit\n",
      "419726    zephyr polishing kit\n",
      "419727    zephyr polishing kit\n",
      "419728    zephyr polishing kit\n",
      "419729    zephyr polishing kit\n",
      "Name: query, Length: 419730, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15048/165082349.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace('[#,@,&,1-9]','')\n"
     ]
    }
   ],
   "source": [
    "#print(text)\n",
    "#text_tokens = word_tokenize(text)\n",
    "text= text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "#remove lowcase\n",
    "text = text.str.lower()\n",
    "#revome caracteres especiais \n",
    "text = text.str.replace('[#,@,&,1-9]','')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando vetores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = train_df['query'].tolist()\n",
    "sentences = text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and NaN\n",
    "#sentences = [word for word in list(set(sentences)) if type(word) is str]\n",
    "sentences = [word for word in list(set(sentences)) if type(word) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick.correia/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# initialize sentence transformer model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens') \n",
    "#paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20815, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sentence embeddings\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndexFlatL2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = sentence_embeddings.shape[1]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20815"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste\n",
    "k = 4\n",
    "xq = model.encode([\"envelopes without security tint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19805  4829   978 14630]]\n",
      "CPU times: user 19.1 ms, sys: 35 µs, total: 19.1 ms\n",
      "Wall time: 17.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19805                6xl knee sleeve with straps\n",
       "4829     1 ply disposable mask without nose clip\n",
       "978              'module' object is not callable\n",
       "14630                            2x3 white board\n",
       "Name: query, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['query'].iloc[[19805, 4829, 978, 14630]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 4 vectors to return (k) - so we initialize a zero array to hold them\n",
    "vecs = np.zeros((k, size))\n",
    "# then iterate through each ID from I and add the reconstructed vector to our zero-array\n",
    "for i, val in enumerate(I[0].tolist()):\n",
    "    vecs[i, :] = index.reconstruct(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43119857,  0.91011864,  0.4051747 , -0.06226606,  0.09338088,\n",
       "       -0.20125297,  1.1626184 , -0.08322101, -0.36759871,  0.20486532,\n",
       "       -0.93038219, -0.61459619,  0.4171924 ,  0.51536101,  0.88794357,\n",
       "        0.45378926, -0.57322139,  0.27390629,  0.29525098, -0.19188608,\n",
       "       -0.24476814,  0.57090127,  0.04908442, -1.19785392, -0.54007924,\n",
       "       -0.35953718,  0.39977959, -0.30137578, -1.15571201,  0.45229897,\n",
       "        0.232692  ,  0.55188435,  1.23124671,  0.08548499,  1.41238093,\n",
       "       -0.40255302, -0.1310015 , -1.07996678, -0.15451427, -0.60550433,\n",
       "       -0.06073511,  0.32497635,  0.32338566,  0.35931736,  0.13488162,\n",
       "       -0.05715447,  1.5942353 ,  0.02342471, -0.78752214,  0.07201277,\n",
       "        0.56621379, -0.50664479,  1.9796375 ,  1.1744287 , -0.15058279,\n",
       "       -0.67751205,  0.21461996, -0.77990431,  0.93161666, -0.23863807,\n",
       "        0.13793547,  0.04795064, -0.26176697,  0.12465028, -0.2205309 ,\n",
       "       -0.32104367,  0.07732798,  0.5417518 , -0.53027701, -1.18944609,\n",
       "        0.67966878, -0.5132727 , -0.14703546, -0.69612658, -0.98438799,\n",
       "        0.2500239 ,  0.58343023,  0.5823316 , -0.31648138,  0.29999939,\n",
       "        0.20667967,  0.54710138,  0.16938254, -0.61964518, -0.57100695,\n",
       "        0.13071777,  0.50421041, -0.23905215, -0.47290611, -0.06526265,\n",
       "        0.51305205,  0.67895544, -0.10349973, -0.13229042, -1.06356549,\n",
       "        0.15988703, -0.43887404,  1.17856085,  0.13287328,  0.24475218])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementando um indice plano faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x = faiss.IndexFlatIP(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = model.encode([\"car paint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10365 12349  7469  7340 11175    50  2151 10218  2665 12706]]\n"
     ]
    }
   ],
   "source": [
    "#Teste\n",
    "k = 10\n",
    "D, I = index_x.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10365          12 pc pot and pan set\n",
       "12349                1u blank vented\n",
       "7469       10 oz tumbler without lid\n",
       "7340     10 lb weights dumbbells set\n",
       "Name: query, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['query'].iloc[[10365, 12349, 7469, 7340]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12349  7469  9072 10365  4282  2151  7340    50 11175  2665]]\n"
     ]
    }
   ],
   "source": [
    "nbits = size*4  # resolution of bucketed vectors\n",
    "# initialize index and add vectors\n",
    "index_e = faiss.IndexLSH(size, nbits)\n",
    "index_e.add(sentence_embeddings)\n",
    "# and search\n",
    "D, I = index_e.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12349                                   1u blank vented\n",
       "7469                          10 oz tumbler without lid\n",
       "9072                   10x 13 picture without the frame\n",
       "10365                             12 pc pot and pan set\n",
       "4282     1 inflatable light up pool ball without remote\n",
       "2151                           00 gauge funny christmas\n",
       "7340                        10 lb weights dumbbells set\n",
       "50                                           # mom life\n",
       "11175                                   13.5 shoes boys\n",
       "2665      08 bmw x5 a/c air compressor without adaptive\n",
       "Name: query, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['query'].iloc[[12349, 7469, 9072, 10365, 4282, 2151, 7340, 50, 11175, 2665 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12349  7469 10365  9072  2665  7340 11175 10218  2151  1005]]\n"
     ]
    }
   ],
   "source": [
    "# set HNSW index parameters\n",
    "M = 64  # number of connections each vertex will have\n",
    "ef_search = 32  # depth of layers explored during search\n",
    "ef_construction = 64  # depth of layers explored during index construction\n",
    "\n",
    "# initialize index (d == 128)\n",
    "index_h = faiss.IndexHNSWFlat(size, M)\n",
    "# set efConstruction and efSearch parameters\n",
    "index_h.hnsw.efConstruction = ef_construction\n",
    "index_h.hnsw.efSearch = ef_search\n",
    "# add data to index\n",
    "index_h.add(xq)\n",
    "\n",
    "# search as usual\n",
    "D, I = index.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
