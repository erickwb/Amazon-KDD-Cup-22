{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "-> Objetivo: Dada uma consulta especificada pelo usuário e uma lista de produtos correspondentes, o objetivo desta tarefa é classificar os produtos para que os produtos relevantes sejam classificados acima dos não relevantes.\n",
    "\n",
    "A entrada para esta tarefa será uma lista de consultas com seus identificadores. O sistema terá que gerar um arquivo CSV onde o **query_id** estará na primeira coluna e o **product_id** na segunda coluna, onde para cada **query_id** primeira linha será o produto mais relevante e a última linha o produto menos relevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximos Passos \n",
    "\n",
    "1 Criar seçao de metricas\n",
    "\n",
    "2 criar modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: lightgbm in /home/erick.correia/.local/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: numpy in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.22.3)\n",
      "Requirement already satisfied: scipy in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: annoy in /home/erick.correia/.local/lib/python3.8/site-packages (1.17.1)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: faiss-cpu in /home/erick.correia/.local/lib/python3.8/site-packages (1.7.2)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already up-to-date: sentence-transformers in /home/erick.correia/.local/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=4.6.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (4.21.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.22.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub>=0.4.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from sentence-transformers) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.8.17)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/erick.correia/.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/erick.correia/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision->sentence-transformers) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/erick.correia/.local/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /home/erick.correia/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: nltk in /home/erick.correia/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (2022.8.17)\n",
      "Requirement already satisfied: tqdm in /home/erick.correia/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install annoy\n",
    "!pip install faiss-cpu\n",
    "!pip install -U sentence-transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "#metricas\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "#modelos\n",
    "import lightgbm as lgb\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train-v0.3.csv')\n",
    "product = pd.read_csv('dataset/product_catalogue-v0.3.csv')\n",
    "teste_df = pd.read_csv('dataset/test_public-v0.3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train data =  (781744, 5)\n",
      "shape train product =  (883868, 7)\n",
      "shape teste product =  (48696, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape train data = \" , train_df.shape)\n",
    "print(\"shape train product = \" , product.shape)\n",
    "print(\"shape teste product = \" , teste_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train product =  (781744, 7)\n"
     ]
    }
   ],
   "source": [
    "full_train = pd.merge(train_df, \n",
    "                          product[[\"product_id\", \"product_locale\", \"product_title\"]],\n",
    "                          left_on=[\"product_id\", \"query_locale\"], \n",
    "                          right_on=[\"product_id\", \"product_locale\"]\n",
    ")\n",
    "print(\"shape train product = \" , full_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train['id']=full_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train product =  (781644, 8)\n"
     ]
    }
   ],
   "source": [
    "full_train = full_train.dropna()\n",
    "print(\"shape train product = \" , full_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_id          0\n",
       "query             0\n",
       "query_locale      0\n",
       "product_id        0\n",
       "esci_label        0\n",
       "product_locale    0\n",
       "product_title     0\n",
       "id                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_train.info()\n",
    "#full_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removendo StopWords / lowcase / caracteres especiais** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (full_train['product_locale'] == 'us')\n",
    "full_train = full_train[mask]\n",
    "full_train = full_train.iloc[:500,:]\n",
    "#full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_locale</th>\n",
       "      <th>product_id</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>product_title</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [query_id, query, query_locale, product_id, esci_label, product_locale, product_title, id]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = full_train.iloc[500:1000,:]\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = full_train['product_title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizando text\n",
    "#text_tokens = word_tokenize(text)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "text= text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9838/4110833427.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace('[!,#,@,&,%,0-9]','')\n"
     ]
    }
   ],
   "source": [
    "text = text.str.lower()\n",
    "#revome caracteres especiais \n",
    "text = text.str.replace('[!,#,@,&,%,0-9]','')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando vetores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentences = train_df['query'].tolist()\n",
    "sentences = text.tolist()\n",
    "len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates and NaN\n",
    "#sentences = [word for word in list(set(sentences)) if type(word) is str]\n",
    "#sentences = [word for word in list(set(sentences)) if type(word) is str]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# initialize sentence transformer model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens') \n",
    "#paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 768)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sentence embeddings\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "faiss.normalize_L2(sentence_embeddings) ## Normalising the Embeddings\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamento de dados de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndexFlatL2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = sentence_embeddings.shape[1]\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(size)\n",
    "index.is_trained\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 500 points to 50 centroids: please provide at least 1950 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 500 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## We get a 768 dimension vector using Roberta. So we will create FAISS index with dimaensions - 768\n",
    "\n",
    "dim=size\n",
    "ncentroids=50 ## This is a hyperparameter, and indicates number of clusters to be split into\n",
    "m=16 ## This is also a hyper parameter\n",
    "quantiser = faiss.IndexFlatL2(dim)\n",
    "index = faiss.IndexIVFPQ (quantiser, dim,ncentroids, m , 8)\n",
    "index.train(sentence_embeddings) ## This step, will do the clustering and create the clusters\n",
    "print(index.is_trained)\n",
    "faiss.write_index(index, \"trained.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "### We have to add the embeddings to the Trained Index.\n",
    "ids=full_train['id'].tolist()\n",
    "\n",
    "ids=np.array(ids)\n",
    "index.add_with_ids(sentence_embeddings,ids)\n",
    "print(index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste\n",
    "k = 4\n",
    "xq = model.encode([\"envelopes without security tint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[485 486 484 477]]\n",
      "CPU times: user 0 ns, sys: 1.81 ms, total: 1.81 ms\n",
      "Wall time: 1.14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInnerProduct(L2_score):\n",
    "    return (2-math.pow(L2_score,2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchFAISSIndex(data,id_col_name,query,index,nprobe,model,topk=20):\n",
    "    ## Convert the query into embeddings\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    dim=query_embedding.shape[0]\n",
    "    query_embedding=query_embedding.reshape(1,dim)\n",
    "    #faiss.normalize_L2(query_embedding)\n",
    "  \n",
    "    \n",
    "    index.nprobe=nprobe\n",
    "    \n",
    "    D,I=index.search(query_embedding,topk) \n",
    "    ids=[i for i in I][0]\n",
    "    L2_score=[d for d in D][0]\n",
    "    inner_product=[calculateInnerProduct(l2) for l2 in L2_score]\n",
    "    search_result=pd.DataFrame()\n",
    "    search_result[id_col_name]=ids\n",
    "    search_result['cosine_sim']=inner_product\n",
    "    search_result['L2_score']=L2_score\n",
    "    print(search_result)\n",
    "    dat=data[data[id_col_name].isin(ids)]\n",
    "    dat=pd.merge(dat,search_result,on=id_col_name)\n",
    "    dat=dat.sort_values('cosine_sim',ascending=False)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id    cosine_sim    L2_score\n",
      "0   468 -30884.525610  248.537827\n",
      "1   473 -31706.057662  251.821594\n",
      "2   497 -31937.215280  252.737869\n",
      "3    31 -32128.078806  253.491928\n",
      "4    32 -32128.078806  253.491928\n",
      "5    30 -32128.078806  253.491928\n",
      "6   496 -32186.449902  253.722092\n",
      "7   467 -32249.520270  253.970551\n",
      "8   466 -32249.520270  253.970551\n",
      "9     5 -32297.556776  254.159622\n",
      "10    6 -32297.556776  254.159622\n",
      "11   17 -32309.894407  254.208160\n",
      "12   16 -32309.894407  254.208160\n",
      "13   15 -32309.894407  254.208160\n",
      "14  499 -32330.296808  254.288406\n",
      "15  475 -32368.356588  254.438034\n",
      "16   12 -32382.311495  254.492874\n",
      "17   10 -32382.311495  254.492874\n",
      "18   14 -32382.311495  254.492874\n",
      "19   13 -32382.311495  254.492874\n"
     ]
    }
   ],
   "source": [
    "query=\"small price tags with string\"\n",
    "search_result=searchFAISSIndex(full_train,\"id\",query,index,nprobe=10,model=model,topk=20)\n",
    "search_result=search_result[['id','product_id','product_title','esci_label','cosine_sim','L2_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>L2_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>468</td>\n",
       "      <td>B001E6C3AE</td>\n",
       "      <td>Avery 12201 Medium-Weight White Marking Tags, ...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-30884.525610</td>\n",
       "      <td>248.537827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>473</td>\n",
       "      <td>B00I0EU22Y</td>\n",
       "      <td>Merchandise Tags #8, White, Scalloped, Hole (n...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-31706.057662</td>\n",
       "      <td>251.821594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>497</td>\n",
       "      <td>B0772W8DPP</td>\n",
       "      <td>YEJI 100 pcs Kraft Paper Tags, Gift Tags with ...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>-31937.215280</td>\n",
       "      <td>252.737869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31</td>\n",
       "      <td>B00OFNI9VK</td>\n",
       "      <td>Ticonderoga Wood-Cased Pencils, #2 HB Soft, Pr...</td>\n",
       "      <td>substitute</td>\n",
       "      <td>-32128.078806</td>\n",
       "      <td>253.491928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>B00OFNI9VK</td>\n",
       "      <td>Ticonderoga Wood-Cased Pencils, #2 HB Soft, Pr...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-32128.078806</td>\n",
       "      <td>253.491928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>B00OFNI9VK</td>\n",
       "      <td>Ticonderoga Wood-Cased Pencils, #2 HB Soft, Pr...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-32128.078806</td>\n",
       "      <td>253.491928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>496</td>\n",
       "      <td>B076BBJYRN</td>\n",
       "      <td>White Paper Tags, Jewelry Price Tags with stri...</td>\n",
       "      <td>substitute</td>\n",
       "      <td>-32186.449902</td>\n",
       "      <td>253.722092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>467</td>\n",
       "      <td>B001E682C2</td>\n",
       "      <td>AVERY White Marking Tags Strung, Pack of 1000 ...</td>\n",
       "      <td>substitute</td>\n",
       "      <td>-32249.520270</td>\n",
       "      <td>253.970551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>466</td>\n",
       "      <td>B001E682C2</td>\n",
       "      <td>AVERY White Marking Tags Strung, Pack of 1000 ...</td>\n",
       "      <td>substitute</td>\n",
       "      <td>-32249.520270</td>\n",
       "      <td>253.970551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>B0002LCZV4</td>\n",
       "      <td>TICONDEROGA Tri-Write Triangular Pencils, Stan...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-32297.556776</td>\n",
       "      <td>254.159622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>B0002LCZV4</td>\n",
       "      <td>TICONDEROGA Tri-Write Triangular Pencils, Stan...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-32297.556776</td>\n",
       "      <td>254.159622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>B001AZ1D3C</td>\n",
       "      <td>Ticonderoga Pencils, Wood-Cased Graphite #2 HB...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-32309.894407</td>\n",
       "      <td>254.208160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>B001AZ1D3C</td>\n",
       "      <td>Ticonderoga Pencils, Wood-Cased Graphite #2 HB...</td>\n",
       "      <td>substitute</td>\n",
       "      <td>-32309.894407</td>\n",
       "      <td>254.208160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>B001AZ1D3C</td>\n",
       "      <td>Ticonderoga Pencils, Wood-Cased Graphite #2 HB...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-32309.894407</td>\n",
       "      <td>254.208160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>499</td>\n",
       "      <td>B078498GPV</td>\n",
       "      <td>Brothersbox Price Tags with String Attached, 5...</td>\n",
       "      <td>substitute</td>\n",
       "      <td>-32330.296808</td>\n",
       "      <td>254.288406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>475</td>\n",
       "      <td>B00SSCGE6Q</td>\n",
       "      <td>Avery Textured White Scallop Round Paper Tags,...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>-32368.356588</td>\n",
       "      <td>254.438034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>B00125Q75Y</td>\n",
       "      <td>TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...</td>\n",
       "      <td>complement</td>\n",
       "      <td>-32382.311495</td>\n",
       "      <td>254.492874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>B00125Q75Y</td>\n",
       "      <td>TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>-32382.311495</td>\n",
       "      <td>254.492874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>B00125Q75Y</td>\n",
       "      <td>TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...</td>\n",
       "      <td>exact</td>\n",
       "      <td>-32382.311495</td>\n",
       "      <td>254.492874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>B00125Q75Y</td>\n",
       "      <td>TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...</td>\n",
       "      <td>substitute</td>\n",
       "      <td>-32382.311495</td>\n",
       "      <td>254.492874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  product_id                                      product_title  \\\n",
       "14  468  B001E6C3AE  Avery 12201 Medium-Weight White Marking Tags, ...   \n",
       "15  473  B00I0EU22Y  Merchandise Tags #8, White, Scalloped, Hole (n...   \n",
       "18  497  B0772W8DPP  YEJI 100 pcs Kraft Paper Tags, Gift Tags with ...   \n",
       "10   31  B00OFNI9VK  Ticonderoga Wood-Cased Pencils, #2 HB Soft, Pr...   \n",
       "11   32  B00OFNI9VK  Ticonderoga Wood-Cased Pencils, #2 HB Soft, Pr...   \n",
       "9    30  B00OFNI9VK  Ticonderoga Wood-Cased Pencils, #2 HB Soft, Pr...   \n",
       "17  496  B076BBJYRN  White Paper Tags, Jewelry Price Tags with stri...   \n",
       "13  467  B001E682C2  AVERY White Marking Tags Strung, Pack of 1000 ...   \n",
       "12  466  B001E682C2  AVERY White Marking Tags Strung, Pack of 1000 ...   \n",
       "1     6  B0002LCZV4  TICONDEROGA Tri-Write Triangular Pencils, Stan...   \n",
       "0     5  B0002LCZV4  TICONDEROGA Tri-Write Triangular Pencils, Stan...   \n",
       "8    17  B001AZ1D3C  Ticonderoga Pencils, Wood-Cased Graphite #2 HB...   \n",
       "7    16  B001AZ1D3C  Ticonderoga Pencils, Wood-Cased Graphite #2 HB...   \n",
       "6    15  B001AZ1D3C  Ticonderoga Pencils, Wood-Cased Graphite #2 HB...   \n",
       "19  499  B078498GPV  Brothersbox Price Tags with String Attached, 5...   \n",
       "16  475  B00SSCGE6Q  Avery Textured White Scallop Round Paper Tags,...   \n",
       "5    14  B00125Q75Y  TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...   \n",
       "4    13  B00125Q75Y  TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...   \n",
       "3    12  B00125Q75Y  TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...   \n",
       "2    10  B00125Q75Y  TICONDEROGA Pencils, Wood-Cased, Unsharpened, ...   \n",
       "\n",
       "    esci_label    cosine_sim    L2_score  \n",
       "14       exact -30884.525610  248.537827  \n",
       "15       exact -31706.057662  251.821594  \n",
       "18  irrelevant -31937.215280  252.737869  \n",
       "10  substitute -32128.078806  253.491928  \n",
       "11       exact -32128.078806  253.491928  \n",
       "9        exact -32128.078806  253.491928  \n",
       "17  substitute -32186.449902  253.722092  \n",
       "13  substitute -32249.520270  253.970551  \n",
       "12  substitute -32249.520270  253.970551  \n",
       "1        exact -32297.556776  254.159622  \n",
       "0        exact -32297.556776  254.159622  \n",
       "8        exact -32309.894407  254.208160  \n",
       "7   substitute -32309.894407  254.208160  \n",
       "6        exact -32309.894407  254.208160  \n",
       "19  substitute -32330.296808  254.288406  \n",
       "16  irrelevant -32368.356588  254.438034  \n",
       "5   complement -32382.311495  254.492874  \n",
       "4   irrelevant -32382.311495  254.492874  \n",
       "3        exact -32382.311495  254.492874  \n",
       "2   substitute -32382.311495  254.492874  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kc store fixtures   merchandise tag without string /\" x -/\" white (pack )'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df['query'].iloc[[19805, 4829, 978, 14630]]\n",
    "#10620 14711  1269 15458\n",
    "sentences[485]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::DirectMap::idx_t faiss::DirectMap::get(faiss::DirectMap::idx_t) const at /project/faiss/faiss/invlists/DirectMap.cpp:81: direct map not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/erick.correia/Área de Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erick.correia/%C3%81rea%20de%20Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# then iterate through each ID from I and add the reconstructed vector to our zero-array\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erick.correia/%C3%81rea%20de%20Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, val \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(I[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/erick.correia/%C3%81rea%20de%20Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     vecs[i, :] \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mreconstruct(val)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/faiss/__init__.py:427\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_reconstruct\u001b[0;34m(self, key, x)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md, )\n\u001b[0;32m--> 427\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreconstruct_c(key, swig_ptr(x))\n\u001b[1;32m    428\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/faiss/swigfaiss.py:4717\u001b[0m, in \u001b[0;36mIndexIVF.reconstruct\u001b[0;34m(self, key, recons)\u001b[0m\n\u001b[1;32m   4715\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreconstruct\u001b[39m(\u001b[39mself\u001b[39m, key, recons):\n\u001b[1;32m   4716\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\" reconstruct a vector. Works only if maintain_direct_map is set to 1 or 2\"\"\"\u001b[39;00m\n\u001b[0;32m-> 4717\u001b[0m     \u001b[39mreturn\u001b[39;00m _swigfaiss\u001b[39m.\u001b[39;49mIndexIVF_reconstruct(\u001b[39mself\u001b[39;49m, key, recons)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::DirectMap::idx_t faiss::DirectMap::get(faiss::DirectMap::idx_t) const at /project/faiss/faiss/invlists/DirectMap.cpp:81: direct map not initialized"
     ]
    }
   ],
   "source": [
    "# we have 4 vectors to return (k) - so we initialize a zero array to hold them\n",
    "vecs = np.zeros((k, size))\n",
    "# then iterate through each ID from I and add the reconstructed vector to our zero-array\n",
    "for i, val in enumerate(I[0].tolist()):\n",
    "    vecs[i, :] = index.reconstruct(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.62268680e-01,  7.02288866e-01,  1.60046089e+00, -2.52886891e-01,\n",
       "        6.94889724e-02,  3.85936290e-01,  1.86582851e+00,  1.95366383e-01,\n",
       "       -1.22308277e-01,  4.64537621e-01, -1.35619247e+00, -5.33786297e-01,\n",
       "        5.25341094e-01,  6.70681655e-01,  1.05484235e+00,  3.25322360e-01,\n",
       "       -7.39459693e-01, -2.09946677e-01,  1.82559446e-01, -2.91459441e-01,\n",
       "       -6.66369274e-02,  2.44928196e-01,  3.32280397e-01, -1.15091193e+00,\n",
       "       -2.93901235e-01, -1.78813443e-01,  2.95608163e-01, -3.64545673e-01,\n",
       "       -1.53809154e+00,  2.25750789e-01,  1.12063624e-01,  4.58471775e-01,\n",
       "        6.88506067e-01,  3.36156100e-01,  1.23908556e+00, -1.64252013e-01,\n",
       "        8.09108242e-02, -1.37895525e+00, -1.68415979e-01, -6.49951220e-01,\n",
       "       -5.58319427e-02,  6.10021830e-01,  6.05048060e-01,  3.45182210e-01,\n",
       "        3.01262170e-01, -1.40101707e-03,  1.93336523e+00, -1.23036504e-01,\n",
       "        3.83745097e-02, -7.41975367e-01, -9.59856808e-02, -4.04200792e-01,\n",
       "        1.97196960e+00,  1.16135526e+00, -1.24976523e-01, -1.09978104e+00,\n",
       "        7.07949936e-01, -8.30358803e-01,  9.14981604e-01, -4.49806498e-03,\n",
       "       -3.80727276e-02,  2.86801040e-01,  3.05205840e-03,  2.20699832e-01,\n",
       "       -6.07862055e-01, -3.22900921e-01,  1.96468141e-02,  3.31938773e-01,\n",
       "       -2.20148608e-01, -9.56506252e-01,  6.52663648e-01, -5.71459115e-01,\n",
       "       -1.71113849e-01, -1.27078608e-01, -1.02696955e+00,  3.01545531e-01,\n",
       "        5.23581326e-01,  6.84444487e-01, -8.54289308e-02,  3.00417632e-01,\n",
       "        4.52195436e-01,  4.03595686e-01,  4.87765431e-01, -8.66071939e-01,\n",
       "       -4.41980094e-01,  1.42826373e-02,  4.76856709e-01, -4.43629235e-01,\n",
       "       -9.41666424e-01, -2.17574611e-02,  4.48552407e-02,  7.81608760e-01,\n",
       "       -1.78716719e-01, -3.18781100e-02, -1.37289083e+00,  4.90883112e-01,\n",
       "       -4.49478239e-01,  8.60628605e-01,  2.67588794e-01,  1.88224260e-02])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementando um indice plano faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x = faiss.IndexFlatIP(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_x.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = model.encode([\"car paint\", \"arsoft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[289056 122675 335018 312637 103322  42819 195218 141480 226699 236514]\n",
      " [ 26941 285127  74533 304948 181771 158355 232285 171911  30746 136225]]\n"
     ]
    }
   ],
   "source": [
    "#Teste\n",
    "k = 10\n",
    "D, I = index_x.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'over under'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df['query'].iloc[[10365, 12349, 7469, 7340]]\n",
    "#12516 15213  7553 12372 18626 15555 11820 20000  5935  4487\n",
    "sentences[285127]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/erick.correia/Área de Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb Cell 51\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/erick.correia/%C3%81rea%20de%20Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m nbits \u001b[39m=\u001b[39m size\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m  \u001b[39m# resolution of bucketed vectors\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erick.correia/%C3%81rea%20de%20Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# initialize index and add vectors\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erick.correia/%C3%81rea%20de%20Trabalho/Estudos/Amazon-KDD-Cup-22/Task_1.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m index_e \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mIndexLSH(size, nbits)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'size' is not defined"
     ]
    }
   ],
   "source": [
    "nbits = size*4  # resolution of bucketed vectors\n",
    "# initialize index and add vectors\n",
    "index_e = faiss.IndexLSH(size, nbits)\n",
    "index_e.add(sentence_embeddings)\n",
    "# and search\n",
    "D, I = index_e.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kwikset 00-0 signature series deadbolt pack of  satin nickel'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df['query'].iloc[[12349, 7469, 9072, 10365, 4282, 2151, 7340, 50, 11175, 2665 ]]\n",
    "#[15213  7553  2488 12516  4900 11820 12372 15555 18626  5935\n",
    "sentences[4900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[117613  49427 218312 285342 324777  98247 209897 174188  43960 171233]\n",
      " [175480 135044 291961  59109 342169 110316  36749 195076  81267 231094]]\n"
     ]
    }
   ],
   "source": [
    "# set HNSW index parameters\n",
    "M = 64  # number of connections each vertex will have\n",
    "ef_search = 32  # depth of layers explored during search\n",
    "ef_construction = 64  # depth of layers explored during index construction\n",
    "\n",
    "# initialize index (d == 128)\n",
    "index_h = faiss.IndexHNSWFlat(size, M)\n",
    "# set efConstruction and efSearch parameters\n",
    "index_h.hnsw.efConstruction = ef_construction\n",
    "index_h.hnsw.efSearch = ef_search\n",
    "# add data to index\n",
    "index_h.add(xq)\n",
    "\n",
    "# search as usual\n",
    "D, I = index.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dellytop womens casual tank tops button up v neck plain sleeveless shirts summer tees army green'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
